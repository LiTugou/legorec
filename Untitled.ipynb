{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f5eca9e-5fff-4f55-b93c-29bb814b0460",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OMP: Info #155: KMP_AFFINITY: Initial OS proc set respected: 0-11\n",
      "OMP: Info #216: KMP_AFFINITY: decoding x2APIC ids.\n",
      "OMP: Info #157: KMP_AFFINITY: 12 available OS procs\n",
      "OMP: Info #158: KMP_AFFINITY: Uniform topology\n",
      "OMP: Info #287: KMP_AFFINITY: topology layer \"LL cache\" is equivalent to \"socket\".\n",
      "OMP: Info #192: KMP_AFFINITY: 1 socket x 6 cores/socket x 2 threads/core (6 total cores)\n",
      "OMP: Info #218: KMP_AFFINITY: OS proc to physical thread map:\n",
      "OMP: Info #172: KMP_AFFINITY: OS proc 0 maps to socket 0 core 0 thread 0 \n",
      "OMP: Info #172: KMP_AFFINITY: OS proc 1 maps to socket 0 core 0 thread 1 \n",
      "OMP: Info #172: KMP_AFFINITY: OS proc 2 maps to socket 0 core 1 thread 0 \n",
      "OMP: Info #172: KMP_AFFINITY: OS proc 3 maps to socket 0 core 1 thread 1 \n",
      "OMP: Info #172: KMP_AFFINITY: OS proc 4 maps to socket 0 core 2 thread 0 \n",
      "OMP: Info #172: KMP_AFFINITY: OS proc 5 maps to socket 0 core 2 thread 1 \n",
      "OMP: Info #172: KMP_AFFINITY: OS proc 6 maps to socket 0 core 4 thread 0 \n",
      "OMP: Info #172: KMP_AFFINITY: OS proc 7 maps to socket 0 core 4 thread 1 \n",
      "OMP: Info #172: KMP_AFFINITY: OS proc 8 maps to socket 0 core 5 thread 0 \n",
      "OMP: Info #172: KMP_AFFINITY: OS proc 9 maps to socket 0 core 5 thread 1 \n",
      "OMP: Info #172: KMP_AFFINITY: OS proc 10 maps to socket 0 core 6 thread 0 \n",
      "OMP: Info #172: KMP_AFFINITY: OS proc 11 maps to socket 0 core 6 thread 1 \n",
      "OMP: Info #254: KMP_AFFINITY: pid 640886 tid 640886 thread 0 bound to OS proc set 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/litugou/Documents/program/tvsearch/base/hook.py:17: The name tf.train.SessionRunHook is deprecated. Please use tf.estimator.SessionRunHook instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/litugou/Documents/program/tvsearch/base/hook.py:64: The name tf.train.CheckpointSaverHook is deprecated. Please use tf.estimator.CheckpointSaverHook instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from base.options import FLAGS,Options\n",
    "from base.features import FeatureConf\n",
    "from base.hook import CheckpointSaverHook\n",
    "\n",
    "if tf.__version__.split(\".\",1)[0] == \"1\":\n",
    "    from tensorflow.keras import layers\n",
    "    tf.data.AUTOTUNE=tf.data.experimental.AUTOTUNE\n",
    "else:\n",
    "    print(tf.__version__.split(\".\",1)[0])\n",
    "    from tensorflow.keras import layers\n",
    "    tf.truncated_normal_initializer=tf.compat.v1.truncated_normal_initializer\n",
    "    tf.feature_column.shared_embedding_columns=tf.feature_column.shared_embeddings\n",
    "    tf.feature_column.input_layer=tf.compat.v1.feature_column.input_layer\n",
    "    tf.losses.log_loss=tf.compat.v1.losses.log_loss\n",
    "    tf.metrics.auc=tf.compat.v1.losses.log_loss\n",
    "    tf.train.get_global_step=tf.compat.v1.train.get_global_step\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f773669e-6790-46f8-9546-eeef0d6063c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from layers import MMoELayer,MLP,SELayer,FMCrossLayer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fc6328e0-7565-4a35-9997-17668d0b427c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepModel():\n",
    "    def __init__(self, opt):\n",
    "        self.opt=opt\n",
    "        feat_obj=FeatureConf(opt.feature_config_path,opt.feature_type_path,opt.KV_map_path,opt.field_emb_config_path)\n",
    "        self.feat_columns=feat_obj.feat_columns\n",
    "        self.feat_input=feat_obj.feat_input\n",
    "        self.feat_schema=feat_obj.feat_schema\n",
    "        self.feat_default=feat_obj.feat_default\n",
    "        \n",
    "        self.params={\n",
    "            'lr': self.opt.lr,\n",
    "            'min_lr': self.opt.min_lr,\n",
    "            'optimizer': self.opt.optimizer,\n",
    "            'max_train_step': self.opt.max_train_step,\n",
    "            'num_cross_layers': 4\n",
    "        }\n",
    "\n",
    "    def _build_optimizer(self, params, distribute=True):\n",
    "        return tf.train.AdamOptimizer(learning_rate=1e-4, beta1=0.9, beta2=0.999, epsilon=1e-8)\n",
    "\n",
    "        optim_type = params['optimizer'].lower()\n",
    "        max_train_step = params['max_train_step']\n",
    "\n",
    "        cur_step = tf.cast(tf.train.get_global_step(), tf.float32)\n",
    "        init_lr = params['lr']\n",
    "        min_lr = params['min_lr']\n",
    "        \n",
    "        cur_lr = init_lr - cur_step * (init_lr - min_lr) / (max_train_step - 1)\n",
    "\n",
    "        if optim_type == 'adam':\n",
    "            optimizer = tf.train.AdamOptimizer(learning_rate=cur_lr, beta1=0.9, beta2=0.999, epsilon=1e-8)\n",
    "        elif optim_type == 'adagrad':\n",
    "            optimizer = tf.train.AdagradOptimizer(learning_rate=cur_lr, initial_accumulator_value=1e-8)\n",
    "        elif optim_type == 'momentum':\n",
    "            optimizer = tf.train.MomentumOptimizer(learning_rate=cur_lr, momentum=0.95)\n",
    "        elif optim_type == 'ftrl':\n",
    "            optimizer = tf.train.FtrlOptimizer(cur_lr)\n",
    "        else:\n",
    "            optimizer = tf.train.GradientDescentOptimizer(cur_lr)\n",
    "\n",
    "        # SyncReplicasOptimizer for distribution\n",
    "        if distribute:\n",
    "            optimizer = tf.train.SyncReplicasOptimizer(optimizer,\n",
    "                                                       replicas_to_aggregate=self.opt.worker_num,\n",
    "                                                       total_num_replicas=self.opt.worker_num,\n",
    "                                                       use_locking=False, sparse_accumulator_type=\"multi_map\")\n",
    "\n",
    "        return optimizer\n",
    "\n",
    "    def model_fn(self,features, labels, mode, params):\n",
    "        ### input\n",
    "        user_sparse_input = tf.feature_column.input_layer(features, self.feat_columns[\"user_sparse_column_list\"])\n",
    "        item_sparse_input = tf.feature_column.input_layer(features, self.feat_columns[\"item_sparse_column_list\"])\n",
    "        query_sparse_input = tf.feature_column.input_layer(features, self.feat_columns[\"query_sparse_column_list\"])\n",
    "        bias_sparse_input = tf.feature_column.input_layer(features, self.feat_columns[\"bias_sparse_column_list\"])\n",
    "        allemb=tf.concat([user_sparse_input,item_sparse_input,query_sparse_input],axis=1)\n",
    "        \n",
    "        sparse_num=len(self.feat_columns[\"user_sparse_column_list\"])+len(self.feat_columns[\"item_sparse_column_list\"])+len(self.feat_columns[\"query_sparse_column_list\"])\n",
    "\n",
    "    \n",
    "        allemb=SELayer([16])(tf.reshape(allemb,(-1,sparse_num,16)))\n",
    "        allemb=tf.reshape(allemb,(-1,16*sparse_num))\n",
    "        \n",
    "        # #ctr_emb,cvr_emb=MMoELayer(256,4,2)(allemb)\n",
    "        # ctr_fm=FMCrossLayer()(tf.reshape(allemb,(-1,sparse_num,16)))\n",
    "        # cvr_fm=FMCrossLayer()(tf.reshape(allemb,(-1,sparse_num,16)))\n",
    "\n",
    "        ctr_emb,cvr_emb=MMoELayer(4,[256],2,[128])(allemb)\n",
    "        # ctr_emb,cvr_emb=PLELayer(1,4,[256],2,[128],1)(allemb)\n",
    "        ## model\n",
    "        ctr=MLP(1,[256,128,64])(ctr_emb)\n",
    "        #ctr=ctr+ctr_fm\n",
    "        cvr=MLP(1,[256,128,64])(ctr_emb)\n",
    "        #cvr=cvr+cvr_fm\n",
    "        cvr_pred = tf.reshape(tf.sigmoid(cvr, name=\"cvr\"),[-1])\n",
    "        \n",
    "        if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "            ctr_pred = tf.reshape(tf.sigmoid(ctr, name=\"ctr\"),[-1])\n",
    "\n",
    "            predictions = {\n",
    "                'ctr_pred': ctr_pred,\n",
    "                'cvr_pred': cvr_pred,\n",
    "            }\n",
    "            export_outputs = {'serving_default': tf.estimator.export.PredictOutput(predictions)}\n",
    "            return tf.estimator.EstimatorSpec(mode, predictions=predictions, export_outputs=export_outputs)\n",
    "        \n",
    "        bias_ctr=MLP(1,[8,8])(bias_sparse_input)\n",
    "        bias_ctr=layers.Dropout(0.8)(bias_ctr)\n",
    "        # bias_ctr=tf.layers.dropout(bias_ctr, rate=0.8, training=True)\n",
    "        ctr_pred = tf.reshape(tf.sigmoid(ctr+bias_ctr, name=\"ctr\"),[-1])\n",
    "\n",
    "        ground_truth_ctr = tf.reshape(labels[\"ctr\"],[-1])\n",
    "        ground_truth_cvr = tf.reshape(labels[\"cvr\"],[-1])\n",
    "        loss1 = tf.reduce_mean(\n",
    "            tf.losses.log_loss(labels=ground_truth_ctr, predictions=ctr_pred))\n",
    "        ## 只使用点击样本\n",
    "        loss2 = tf.reduce_mean(\n",
    "            tf.losses.log_loss(labels=ground_truth_cvr, predictions=cvr_pred,\n",
    "                               weights=ground_truth_ctr+ground_truth_cvr*20))\n",
    "        # loss2 = tf.reduce_sum(\n",
    "        #     tf.losses.log_loss(labels=ground_truth_cvr, predictions=cvr_pred,\n",
    "        #                        weights=ground_truth_ctr))/tf.reduce_sum(ground_truth_ctr)\n",
    "        loss = loss1 + loss2\n",
    "        tf.add_to_collection('loss',loss)\n",
    "        tf.add_to_collection('loss1',loss1)\n",
    "        tf.add_to_collection('loss2',loss2)\n",
    "        \n",
    "        # eval\n",
    "        auc_ctr = tf.metrics.auc(labels=ground_truth_ctr, predictions=ctr_pred)\n",
    "        auc_cvr = tf.metrics.auc(labels=ground_truth_cvr, predictions=cvr_pred)\n",
    "\n",
    "        # tf.print(\"loss_ctr: \", loss1,\"  loss_cvr:\", loss2)\n",
    "        # print(\"auc_ctr\", auc_ctr[1],\"auc_cvr\", auc_cvr[1])\n",
    "        metrics = {'auc_ctr': auc_ctr,\"auc_cvr\":auc_cvr}\n",
    "\n",
    "        if mode == tf.estimator.ModeKeys.EVAL:\n",
    "            return tf.estimator.EstimatorSpec(mode, loss=loss, eval_metric_ops=metrics)\n",
    "\n",
    "        # train\n",
    "        optimizer = self._build_optimizer(params, distribute=self.opt.distribute)\n",
    "        \n",
    "        training_hooks=[]\n",
    "        training_chief_hooks=[]\n",
    "        \n",
    "        saver_hook = CheckpointSaverHook(checkpoint_dir=self.opt.output_dir,\n",
    "                                                       save_steps=self.opt.save_checkpoint_and_eval_step)\n",
    "        pos_cvr=tf.reduce_sum(ground_truth_cvr)\n",
    "        pos_ctr=tf.reduce_sum(ground_truth_ctr)\n",
    "        logging_hook = tf.train.LoggingTensorHook({\"loss\":loss, \"loss1\": loss1, \"loss2\": loss2,\"auc_ctr\":auc_ctr[1],\"auc_cvr\":auc_cvr[1],\"pos_ctr\":pos_ctr,\"pos_cvr\":pos_cvr}, every_n_iter=100)\n",
    "        if self.opt.distribute:\n",
    "            print(\"hook_sync_replicas is set\")\n",
    "            self.hook_sync_replicas = optimizer.make_session_run_hook(is_chief=self.is_chief, num_tokens=0)\n",
    "            training_hooks.append(self.hook_sync_replicas)\n",
    "            training_chief_hooks.append(saver_hook)\n",
    "            training_chief_hooks.append(loggin_hook)\n",
    "        else:\n",
    "            training_hooks.append(logging_hook)\n",
    "            training_hooks.append(saver_hook)\n",
    "        # train_op = optimizer.minimize(loss)\n",
    "        train_op = optimizer.minimize(loss, global_step=tf.train.get_global_step())\n",
    "        if self.opt.distribute:\n",
    "            self.sync_init_op = optimizer.get_init_tokens_op()\n",
    "        return tf.estimator.EstimatorSpec(mode, loss=loss, train_op=train_op,\n",
    "                                          training_hooks=training_hooks,\n",
    "                                          training_chief_hooks=training_chief_hooks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3a83021d-47a5-46c6-8e42-e23c2fed491c",
   "metadata": {},
   "outputs": [],
   "source": [
    "FLAGS(sys.argv)\n",
    "opt=Options()\n",
    "modelobj=DeepModel(opt)\n",
    "feat_schema=modelobj.feat_schema\n",
    "# feat_schema[\"finalClickFlag\"]=tf.io.FixedLenFeature((), tf.float32)\n",
    "# feat_schema[\"pay_flag\"]=tf.io.FixedLenFeature((), tf.float32)\n",
    "feat_schema[\"finalClickFlag\"]=tf.io.FixedLenFeature((), tf.int64)\n",
    "feat_schema[\"pay_flag\"]=tf.io.FixedLenFeature((), tf.int64)\n",
    "\n",
    "feat_default=modelobj.feat_default\n",
    "feat_default[\"finalClickFlag\"]=0\n",
    "feat_default[\"pay_flag\"]=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f0759f4e-82d1-4eae-86bb-e95c33223ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"data/train/00.csv\",\"r\") as f:\n",
    "#     head=f.readline().strip()\n",
    "# column_names=head.split(\",\")\n",
    "# feat_all=modelobj.feat_default.keys()\n",
    "# for col in columns_names:\n",
    "#     if col not in feat_all:\n",
    "#         print(col)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1251fc33-13ee-4222-b4a3-abff77cfc729",
   "metadata": {},
   "source": [
    "### load from csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d746bdf5-690c-4d2c-acc4-322256df9b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.dataio import *\n",
    "\n",
    "train_list=glob.glob(\"./data/train/*.csv\")\n",
    "val_list=glob.glob(\"./data/val/*.csv\")\n",
    "test_list=glob.glob(\"./data/test/*.csv\")\n",
    "with open(\"data/train/00.csv\",\"r\") as f:\n",
    "        head=f.readline().strip()\n",
    "column_names=head.split(\",\")\n",
    "record_defaults=[feat_default.get(col,\"drop_value\") for col in column_names]\n",
    "drop_columns=[\"uid\",\"normslotmatchscore\",\"normtrunkmatchscore\"]\n",
    "\n",
    "# ds=input_fn(val_list,column_names,record_defaults,drop_columns,batch_size=1024)\n",
    "ds=input_fn(train_list,column_names,record_defaults,drop_columns,batch_size=1024)\n",
    "# batch=next(iter(ds.as_numpy_iterator()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab2e9ba-21c6-47a5-94d5-d0bd8c1badfc",
   "metadata": {},
   "source": [
    "### load from tfrecords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1652e8d7-0c81-4ede-a85f-a763866377bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_list=glob.glob(\"./data/train_tf/*.tf\")\n",
    "val_list=glob.glob(\"./data/val_tf/*.tf\")\n",
    "test_list=glob.glob(\"./data/test_tf/*.tf\")\n",
    "ds=loadtf(train_list,feat_schema,batch_size=64)\n",
    "# batch=next(iter(ds.as_numpy_iterator()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5922342f-fbd3-4e26-9783-80298478edc3",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "source": [
    "### tf1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "af462784-bc57-4253-b723-0a7110ffc5cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using config: {'_model_dir': './tmp_model', '_tf_random_seed': 2020, '_save_summary_steps': 500, '_save_checkpoints_steps': 500, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 1000, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': 2000, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f76e25bef90>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"
     ]
    }
   ],
   "source": [
    "opt.max_train_step=7400\n",
    "opt.output_dir=\"./tmp_model\"\n",
    "opt.save_summary_steps=500\n",
    "opt.save_checkpoint_and_eval_step=500\n",
    "\n",
    "classifier = tf.estimator.Estimator(\n",
    "    model_fn=modelobj.model_fn,\n",
    "    params=modelobj.params,\n",
    "    config=tf.estimator.RunConfig(\n",
    "                model_dir=opt.output_dir,\n",
    "                tf_random_seed=2020,\n",
    "                save_summary_steps=opt.save_summary_steps,\n",
    "                save_checkpoints_steps=opt.save_checkpoint_and_eval_step,\n",
    "                keep_checkpoint_max=1000,\n",
    "                experimental_max_worker_delay_secs=2000)\n",
    ")\n",
    "\n",
    "# train_list=glob.glob(\"./data/train/*.csv\")\n",
    "# val_list=glob.glob(\"./data/val/*.csv\")\n",
    "# test_list=glob.glob(\"./data/test/*.csv\")\n",
    "\n",
    "# train_spec = tf.estimator.TrainSpec(\n",
    "#     input_fn=lambda: input_fn(train_list,column_names,record_defaults,drop_columns,batch_size=1024),\n",
    "#     max_steps=opt.max_train_step\n",
    "# )\n",
    "# eval_spec = tf.estimator.EvalSpec(\n",
    "#     input_fn=lambda: input_fn(val_list,column_names,record_defaults,drop_columns,batch_size=1024),\n",
    "#     steps=100,\n",
    "#     start_delay_secs=3,\n",
    "#     throttle_secs=3\n",
    "# )\n",
    "\n",
    "\n",
    "train_list=glob.glob(\"./data/train_tf/*.tf\")\n",
    "val_list=glob.glob(\"./data/val_tf/*.tf\")\n",
    "test_list=glob.glob(\"./data/test_tf/*.tf\")\n",
    "train_spec = tf.estimator.TrainSpec(\n",
    "    input_fn=lambda: loadtf(train_list,feat_schema,batch_size=1024),\n",
    "    max_steps=opt.max_train_step\n",
    ")\n",
    "eval_spec = tf.estimator.EvalSpec(\n",
    "    input_fn=lambda: loadtf(val_list,feat_schema,batch_size=1024),\n",
    "    steps=1000,\n",
    "    start_delay_secs=3,\n",
    "    throttle_secs=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c22e5a1-5dec-4146-87c6-8349da755f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.estimator.train_and_evaluate(classifier, train_spec, eval_spec)\n",
    "# classifier.evaluate(input_fn=lambda :loadtf(val_list+test_list,feat_schema,num_epochs=1,batch_size=4096))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "219a9368-7888-4463-9285-32311541f91c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
