{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a4662e10",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from layers import *\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "650e78d9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-08 10:09:01.043586: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2023-10-08 10:09:01.043752: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-10-08 10:09:01.044784: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers,activations,backend,constraints,initializers,regularizers\n",
    "\n",
    "class NFM(layers.Layer):\n",
    "    def __init__(self,\n",
    "                 dropout_rate=0.2,\n",
    "                 **kwargs):\n",
    "        self.dropout_rate=dropout_rate\n",
    "        super().__init__(**kwargs)\n",
    "        self.bnlayer=layers.BatchNormalization(name='bi_interaction_bn')\n",
    "        self.dropoutlayer=layers.Dropout(rate=dropout_rate)\n",
    "\n",
    "    def call(self,inputs):\n",
    "        \"\"\"\n",
    "        inputs: (bs,field_num,emb_size)\n",
    "        output: (bs,emb_size)\n",
    "        \"\"\"\n",
    "        sum_square_part = tf.square(tf.reduce_sum(inputs, axis=1)) # (batch, emb_size)\n",
    "        square_sum_part = tf.reduce_sum(tf.square(inputs), axis=1) # (batch, emb_size)\n",
    "        nfm = 0.5 * (sum_square_part - square_sum_part)\n",
    "        nfm = self.bnlayer(nfm)\n",
    "        nfm = self.dropoutlayer(nfm)\n",
    "        return nfm\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    obj=NFM()\n",
    "    a=tf.ones((64,23,16))\n",
    "    obj(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0b98a253",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "a=tf.ones((64,23,16))\n",
    "b=tf.expand_dims(tf.ones((64,23)),axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "316446cd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'keras'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m activations\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m backend\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m constraints\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'keras'"
     ]
    }
   ],
   "source": [
    "import tensorflow.compat.v2 as tf\n",
    "from keras import activations\n",
    "from keras import backend\n",
    "from keras import constraints\n",
    "from keras import initializers\n",
    "from keras import regularizers\n",
    "from keras.engine import base_layer\n",
    "from keras.layers.rnn import rnn_utils\n",
    "from keras.layers.rnn.dropout_rnn_cell_mixin import DropoutRNNCellMixin\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# isort: off\n",
    "from tensorflow.python.platform import tf_logging as logging\n",
    "\n",
    "RECURRENT_DROPOUT_WARNING_MSG = (\n",
    "    \"RNN `implementation=2` is not supported when `recurrent_dropout` is set. \"\n",
    "    \"Using `implementation=1`.\"\n",
    ")\n",
    "\n",
    "\n",
    "class AUGRUCell(DropoutRNNCellMixin, base_layer.BaseRandomLayer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        units,\n",
    "        activation=\"tanh\",\n",
    "        recurrent_activation=\"sigmoid\",\n",
    "        use_bias=True,\n",
    "        kernel_initializer=\"glorot_uniform\",\n",
    "        recurrent_initializer=\"orthogonal\",\n",
    "        bias_initializer=\"zeros\",\n",
    "        kernel_regularizer=None,\n",
    "        recurrent_regularizer=None,\n",
    "        bias_regularizer=None,\n",
    "        kernel_constraint=None,\n",
    "        recurrent_constraint=None,\n",
    "        bias_constraint=None,\n",
    "        dropout=0.0,\n",
    "        recurrent_dropout=0.0,\n",
    "        reset_after=True,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        # By default use cached variable under v2 mode, see b/143699808.\n",
    "        if tf.compat.v1.executing_eagerly_outside_functions():\n",
    "            self._enable_caching_device = kwargs.pop(\n",
    "                \"enable_caching_device\", True\n",
    "            )\n",
    "        else:\n",
    "            self._enable_caching_device = kwargs.pop(\n",
    "                \"enable_caching_device\", False\n",
    "            )\n",
    "        super().__init__(**kwargs)\n",
    "        self.units = units\n",
    "        self.activation = activations.get(activation)\n",
    "        self.recurrent_activation = activations.get(recurrent_activation)\n",
    "        self.use_bias = use_bias\n",
    "\n",
    "        self.kernel_initializer = initializers.get(kernel_initializer)\n",
    "        self.recurrent_initializer = initializers.get(recurrent_initializer)\n",
    "        self.bias_initializer = initializers.get(bias_initializer)\n",
    "\n",
    "        self.kernel_regularizer = regularizers.get(kernel_regularizer)\n",
    "        self.recurrent_regularizer = regularizers.get(recurrent_regularizer)\n",
    "        self.bias_regularizer = regularizers.get(bias_regularizer)\n",
    "\n",
    "        self.kernel_constraint = constraints.get(kernel_constraint)\n",
    "        self.recurrent_constraint = constraints.get(recurrent_constraint)\n",
    "        self.bias_constraint = constraints.get(bias_constraint)\n",
    "\n",
    "        self.dropout = min(1.0, max(0.0, dropout))\n",
    "        self.recurrent_dropout = min(1.0, max(0.0, recurrent_dropout))\n",
    "\n",
    "        implementation = kwargs.pop(\"implementation\", 2)\n",
    "        if self.recurrent_dropout != 0 and implementation != 1:\n",
    "            logging.debug(RECURRENT_DROPOUT_WARNING_MSG)\n",
    "            self.implementation = 1\n",
    "        else:\n",
    "            self.implementation = implementation\n",
    "        self.reset_after = reset_after\n",
    "        self.state_size = self.units\n",
    "        self.output_size = self.units\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        input_shape=input_shape[0]\n",
    "        super().build(input_shape)\n",
    "        input_dim = input_shape[-1]\n",
    "        default_caching_device = rnn_utils.caching_device(self)\n",
    "        self.kernel = self.add_weight(\n",
    "            shape=(input_dim, self.units * 3),\n",
    "            name=\"kernel\",\n",
    "            initializer=self.kernel_initializer,\n",
    "            regularizer=self.kernel_regularizer,\n",
    "            constraint=self.kernel_constraint,\n",
    "            caching_device=default_caching_device,\n",
    "        )\n",
    "        self.recurrent_kernel = self.add_weight(\n",
    "            shape=(self.units, self.units * 3),\n",
    "            name=\"recurrent_kernel\",\n",
    "            initializer=self.recurrent_initializer,\n",
    "            regularizer=self.recurrent_regularizer,\n",
    "            constraint=self.recurrent_constraint,\n",
    "            caching_device=default_caching_device,\n",
    "        )\n",
    "\n",
    "        if self.use_bias:\n",
    "            if not self.reset_after:\n",
    "                bias_shape = (3 * self.units,)\n",
    "            else:\n",
    "                # separate biases for input and recurrent kernels\n",
    "                # Note: the shape is intentionally different from CuDNNGRU\n",
    "                # biases `(2 * 3 * self.units,)`, so that we can distinguish the\n",
    "                # classes when loading and converting saved weights.\n",
    "                bias_shape = (2, 3 * self.units)\n",
    "            self.bias = self.add_weight(\n",
    "                shape=bias_shape,\n",
    "                name=\"bias\",\n",
    "                initializer=self.bias_initializer,\n",
    "                regularizer=self.bias_regularizer,\n",
    "                constraint=self.bias_constraint,\n",
    "                caching_device=default_caching_device,\n",
    "            )\n",
    "        else:\n",
    "            self.bias = None\n",
    "        self.built = True\n",
    "\n",
    "    def call(self, inputs, states, training=None):\n",
    "        inputs, att_score = inputs\n",
    "        h_tm1 = (\n",
    "            states[0] if tf.nest.is_nested(states) else states\n",
    "        )  # previous memory\n",
    "\n",
    "        dp_mask = self.get_dropout_mask_for_cell(inputs, training, count=3)\n",
    "        rec_dp_mask = self.get_recurrent_dropout_mask_for_cell(\n",
    "            h_tm1, training, count=3\n",
    "        )\n",
    "\n",
    "        if self.use_bias:\n",
    "            if not self.reset_after:\n",
    "                input_bias, recurrent_bias = self.bias, None\n",
    "            else:\n",
    "                input_bias, recurrent_bias = tf.unstack(self.bias)\n",
    "\n",
    "        if self.implementation == 1:\n",
    "            if 0.0 < self.dropout < 1.0:\n",
    "                inputs_z = inputs * dp_mask[0]\n",
    "                inputs_r = inputs * dp_mask[1]\n",
    "                inputs_h = inputs * dp_mask[2]\n",
    "            else:\n",
    "                inputs_z = inputs\n",
    "                inputs_r = inputs\n",
    "                inputs_h = inputs\n",
    "\n",
    "            x_z = backend.dot(inputs_z, self.kernel[:, : self.units])\n",
    "            x_r = backend.dot(\n",
    "                inputs_r, self.kernel[:, self.units : self.units * 2]\n",
    "            )\n",
    "            x_h = backend.dot(inputs_h, self.kernel[:, self.units * 2 :])\n",
    "\n",
    "            if self.use_bias:\n",
    "                x_z = backend.bias_add(x_z, input_bias[: self.units])\n",
    "                x_r = backend.bias_add(\n",
    "                    x_r, input_bias[self.units : self.units * 2]\n",
    "                )\n",
    "                x_h = backend.bias_add(x_h, input_bias[self.units * 2 :])\n",
    "\n",
    "            if 0.0 < self.recurrent_dropout < 1.0:\n",
    "                h_tm1_z = h_tm1 * rec_dp_mask[0]\n",
    "                h_tm1_r = h_tm1 * rec_dp_mask[1]\n",
    "                h_tm1_h = h_tm1 * rec_dp_mask[2]\n",
    "            else:\n",
    "                h_tm1_z = h_tm1\n",
    "                h_tm1_r = h_tm1\n",
    "                h_tm1_h = h_tm1\n",
    "\n",
    "            recurrent_z = backend.dot(\n",
    "                h_tm1_z, self.recurrent_kernel[:, : self.units]\n",
    "            )\n",
    "            recurrent_r = backend.dot(\n",
    "                h_tm1_r, self.recurrent_kernel[:, self.units : self.units * 2]\n",
    "            )\n",
    "            if self.reset_after and self.use_bias:\n",
    "                recurrent_z = backend.bias_add(\n",
    "                    recurrent_z, recurrent_bias[: self.units]\n",
    "                )\n",
    "                recurrent_r = backend.bias_add(\n",
    "                    recurrent_r, recurrent_bias[self.units : self.units * 2]\n",
    "                )\n",
    "\n",
    "            z = self.recurrent_activation(x_z + recurrent_z)\n",
    "            r = self.recurrent_activation(x_r + recurrent_r)\n",
    "\n",
    "            # reset gate applied after/before matrix multiplication\n",
    "            if self.reset_after:\n",
    "                recurrent_h = backend.dot(\n",
    "                    h_tm1_h, self.recurrent_kernel[:, self.units * 2 :]\n",
    "                )\n",
    "                if self.use_bias:\n",
    "                    recurrent_h = backend.bias_add(\n",
    "                        recurrent_h, recurrent_bias[self.units * 2 :]\n",
    "                    )\n",
    "                recurrent_h = r * recurrent_h\n",
    "            else:\n",
    "                recurrent_h = backend.dot(\n",
    "                    r * h_tm1_h, self.recurrent_kernel[:, self.units * 2 :]\n",
    "                )\n",
    "\n",
    "            hh = self.activation(x_h + recurrent_h)\n",
    "        else:\n",
    "            if 0.0 < self.dropout < 1.0:\n",
    "                inputs = inputs * dp_mask[0]\n",
    "\n",
    "            # inputs projected by all gate matrices at once\n",
    "            matrix_x = backend.dot(inputs, self.kernel)\n",
    "            if self.use_bias:\n",
    "                # biases: bias_z_i, bias_r_i, bias_h_i\n",
    "                matrix_x = backend.bias_add(matrix_x, input_bias)\n",
    "\n",
    "            x_z, x_r, x_h = tf.split(matrix_x, 3, axis=-1)\n",
    "\n",
    "            if self.reset_after:\n",
    "                # hidden state projected by all gate matrices at once\n",
    "                matrix_inner = backend.dot(h_tm1, self.recurrent_kernel)\n",
    "                if self.use_bias:\n",
    "                    matrix_inner = backend.bias_add(\n",
    "                        matrix_inner, recurrent_bias\n",
    "                    )\n",
    "            else:\n",
    "                # hidden state projected separately for update/reset and new\n",
    "                matrix_inner = backend.dot(\n",
    "                    h_tm1, self.recurrent_kernel[:, : 2 * self.units]\n",
    "                )\n",
    "\n",
    "            recurrent_z, recurrent_r, recurrent_h = tf.split(\n",
    "                matrix_inner, [self.units, self.units, -1], axis=-1\n",
    "            )\n",
    "\n",
    "            z = self.recurrent_activation(x_z + recurrent_z)\n",
    "            r = self.recurrent_activation(x_r + recurrent_r)\n",
    "\n",
    "            if self.reset_after:\n",
    "                recurrent_h = r * recurrent_h\n",
    "            else:\n",
    "                recurrent_h = backend.dot(\n",
    "                    r * h_tm1, self.recurrent_kernel[:, 2 * self.units :]\n",
    "                )\n",
    "\n",
    "            hh = self.activation(x_h + recurrent_h)\n",
    "        # previous and candidate state mixed by update gate\n",
    "        z= z*att_score\n",
    "        h = z * h_tm1 + (1 - z) * hh\n",
    "        new_state = [h] if tf.nest.is_nested(states) else h\n",
    "        return h, new_state\n",
    "\n",
    "    \n",
    "    \n",
    "from layers.sequential.din import DiceMLP\n",
    "class ActivationUnit(layers.Layer):\n",
    "\n",
    "    def __init__(self, units=[32, 16], dropout_rate=0.2):\n",
    "        super().__init__()\n",
    "        self.dicemlp = DiceMLP(1,units,dropout_rate=dropout_rate)\n",
    "\n",
    "    def build(self,input_shape):\n",
    "        self.emb_size=input_shape[2]\n",
    "        self.maxseqlen=input_shape[1]\n",
    "        \n",
    "    def call(self, sequence, query):\n",
    "        \"\"\"\n",
    "            query : 单独的ad的embedding mat -> batch * 1 * embed\n",
    "            user_behavior : 行为特征矩阵 -> batch * seq_len * embed\n",
    "        \"\"\"\n",
    "        key=sequence\n",
    "        query=tf.tile(query,[1,self.maxseqlen]) ## (bs,emb_dim) -> (bs,emb_dim*seqlen)\n",
    "        query=tf.reshape(query,[-1,self.maxseqlen,self.emb_size]) # (bs,emb_dim*seqlen) -> (bs,seqlen,emb_dim)\n",
    "        ## key 和 query的一些交互,如果有其它特征other_emb 可以一并拼接\n",
    "        din_all=tf.concat([query,key,query-key,query*key],axis=-1) ## (bs,seqlen,emb_dim*4)\n",
    "        ## 一个2层mlp (emb_dim*2,emb_dim,1)\n",
    "        din_w=self.dicemlp(din_all)\n",
    "        return din_w\n",
    "\n",
    "'''   Attention Pooling Layer   '''\n",
    "class AttentionPoolingLayer(layers.Layer):\n",
    "\n",
    "    def __init__(self,units=[32, 16], dropout_rate=0.2, return_score=False):\n",
    "        super().__init__()\n",
    "        self.active_unit = ActivationUnit(units,dropout_rate)\n",
    "        self.return_score = return_score\n",
    "\n",
    "    def call(self,user_behavior,query, mask_bool):\n",
    "        \"\"\"\n",
    "            query_ad : 单独的ad的embedding mat -> batch * 1 * embed\n",
    "            user_behavior : 行为特征矩阵 -> batch * seq_len * embed\n",
    "            mask : 被padding为0的行为置为false -> batch * seq_len * 1\n",
    "        \"\"\"\n",
    "\n",
    "        # attn weights\n",
    "        attn_weights = self.active_unit(user_behavior,query)\n",
    "        # mul weights and sum pooling\n",
    "        if self.return_score:\n",
    "            output = user_behavior * attn_weights *tf.expand_dims(tf.cast(mask_bool,tf.float32),axis=-1)\n",
    "            return output\n",
    "\n",
    "        return attn_weights\n",
    "    \n",
    "\n",
    "sequence=tf.ones((64,20,16))\n",
    "query=tf.ones((64,16))\n",
    "mask_bool=tf.sequence_mask(tf.ones((64,))*10,20)\n",
    "obj=ActivationUnit()\n",
    "obj(sequence,query)\n",
    "obj=AttentionPoolingLayer(return_score=True)\n",
    "tmp=obj(sequence,query,mask_bool)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "eb18fe5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(64, 16), dtype=float32, numpy=\n",
       "array([[ 0.01445294, -0.44694364, -0.9463091 , ..., -0.29685327,\n",
       "        -0.70927423,  0.48810744],\n",
       "       [ 0.01445294, -0.44694364, -0.9463091 , ..., -0.29685327,\n",
       "        -0.70927423,  0.48810744],\n",
       "       [ 0.01445294, -0.44694364, -0.9463091 , ..., -0.29685327,\n",
       "        -0.70927423,  0.48810744],\n",
       "       ...,\n",
       "       [ 0.01445294, -0.44694364, -0.9463091 , ..., -0.29685327,\n",
       "        -0.70927423,  0.48810744],\n",
       "       [ 0.01445294, -0.44694364, -0.9463091 , ..., -0.29685327,\n",
       "        -0.70927423,  0.48810744],\n",
       "       [ 0.01445294, -0.44694364, -0.9463091 , ..., -0.29685327,\n",
       "        -0.70927423,  0.48810744]], dtype=float32)>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://blog.csdn.net/qq_42363032/article/details/122365548\n",
    "class InterestEvolutionLayer(layers.Layer):\n",
    "    def __init__(self,\n",
    "                 attention_units=[32,16],\n",
    "                 attention_dropout=0,\n",
    "                 gru_type=\"augru\",\n",
    "                 **kwargs):\n",
    "        self.gru_type=gru_type\n",
    "        self.attention_units=attention_units\n",
    "        self.attention_dropout=attention_dropout\n",
    "        super().__init__(**kwargs)\n",
    "        \n",
    "    def build(self,input_shape):\n",
    "        emb_size=input_shape[-1]\n",
    "        self.maxseqlen=input_shape[1]\n",
    "        if self.gru_type.upper() == \"AUGRU\":\n",
    "            self.attention = AttentionPoolingLayer(units=self.attention_units,\n",
    "                                                   dropout_rate=self.attention_dropout,\n",
    "                                                   return_score=True)\n",
    "            self.rnn=layers.RNN(AUGRUCell(units=emb_size))\n",
    "        elif self.gru_type.upper() == \"GRU\":\n",
    "            self.attention = AttentionPoolingLayer(dropout=self.attention_dropout, units=self.attention_units)\n",
    "            self.rnn=layers.GRU(units=emb_size,return_sequences=True)\n",
    "            \n",
    "        elif self.gru_type.upper() in (\"AIGRU\") :\n",
    "            self.attention = AttentionPoolingLayer(dropout=self.attention_dropout, units=self.attention_units)\n",
    "            self.rnn=layers.GRU(units=emb_size)\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "    def call(self,gru_interests,query_ad,seqlen):\n",
    "        mask_bool=tf.sequence_mask(seqlen,maxlen=self.maxseqlen)\n",
    "        if self.gru_type.upper() == 'GRU':\n",
    "            # GRU后接attention\n",
    "            out = self.rnn(gru_interests, mask=mask_bool)  # (2000, 40, 4)\n",
    "            out = self.attention(out, query_ad, mask_bool)  # (2000, 40, 4)\n",
    "            out = tf.reduce_sum(out, axis=1)  # (2000, 4)\n",
    "        elif self.gru_type.upper() == 'AIGRU':\n",
    "            # AIGRU\n",
    "            att_score = self.attention(gru_interests, query_ad, mask_bool)  # (2000, 40, 1)\n",
    "            out = att_score * gru_interests  # (2000, 40, 4)\n",
    "            out = self.rnn(out, mask=mask_bool)  # (2000, 4)\n",
    "        elif self.gru_type.upper() == 'AUGRU':\n",
    "            # AGRU or AUGRU\n",
    "            att_score = self.attention(gru_interests, query_ad,  mask_bool)  # (2000, 40, 1)\n",
    "            out = self.rnn((gru_interests, att_score), mask=mask_bool)  # (2000, 4)\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "        return out\n",
    "    \n",
    "\n",
    "rnn=layers.RNN(AUGRUCell(units=16),return_sequences=True)\n",
    "inputs=tf.ones((64,20,16))\n",
    "query=tf.ones((64,16))\n",
    "seqlen=tf.ones((64))*10\n",
    "att_score=tf.ones((64,20,1))\n",
    "rnn((inputs,att_score))\n",
    "obj=InterestEvolutionLayer()\n",
    "obj(inputs,query,seqlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cc2cd284",
   "metadata": {},
   "outputs": [],
   "source": [
    "from layers.sequential.din import DiceMLP\n",
    "class ActivationUnit(layers.Layer):\n",
    "\n",
    "    def __init__(self,att_dropout=0.2, att_fc_dims=[32, 16]):\n",
    "        super().__init__()\n",
    "        self.dicemlp = DiceMLP(1,att_fc_dims)\n",
    "\n",
    "    def build(self,input_shape):\n",
    "        self.emb_size=input_shape[2]\n",
    "        self.maxseqlen=input_shape[1]\n",
    "        \n",
    "    def call(self, sequence, query):\n",
    "        \"\"\"\n",
    "            query : 单独的ad的embedding mat -> batch * 1 * embed\n",
    "            user_behavior : 行为特征矩阵 -> batch * seq_len * embed\n",
    "        \"\"\"\n",
    "        key=sequence\n",
    "        query=tf.tile(query,[1,self.maxseqlen]) ## (bs,emb_dim) -> (bs,emb_dim*seqlen)\n",
    "        query=tf.reshape(query,[-1,self.maxseqlen,self.emb_size]) # (bs,emb_dim*seqlen) -> (bs,seqlen,emb_dim)\n",
    "        ## key 和 query的一些交互,如果有其它特征other_emb 可以一并拼接\n",
    "        din_all=tf.concat([query,key,query-key,query*key],axis=-1) ## (bs,seqlen,emb_dim*4)\n",
    "        ## 一个2层mlp (emb_dim*2,emb_dim,1)\n",
    "        din_w=self.dicemlp(din_all)\n",
    "        return din_w\n",
    "\n",
    "'''   Attention Pooling Layer   '''\n",
    "class AttentionPoolingLayer(layers.Layer):\n",
    "\n",
    "    def __init__(self, att_dropout=0.2, att_fc_dims=[32, 16], return_score=False):\n",
    "        super().__init__()\n",
    "        self.active_unit = ActivationUnit(att_dropout, att_fc_dims)\n",
    "        self.return_score = return_score\n",
    "\n",
    "    def call(self,user_behavior,query, mask_bool):\n",
    "        \"\"\"\n",
    "            query_ad : 单独的ad的embedding mat -> batch * 1 * embed\n",
    "            user_behavior : 行为特征矩阵 -> batch * seq_len * embed\n",
    "            mask : 被padding为0的行为置为false -> batch * seq_len * 1\n",
    "        \"\"\"\n",
    "\n",
    "        # attn weights\n",
    "        attn_weights = self.active_unit(user_behavior,query)\n",
    "        # mul weights and sum pooling\n",
    "        if self.return_score:\n",
    "            output = user_behavior * attn_weights *tf.expand_dims(tf.cast(mask_bool,tf.float32),axis=-1)\n",
    "            return output\n",
    "\n",
    "        return attn_weights\n",
    "    \n",
    "\n",
    "sequence=tf.ones((64,20,16))\n",
    "query=tf.ones((64,16))\n",
    "mask_bool=tf.sequence_mask(tf.ones((64,))*10,20)\n",
    "obj=ActivationUnit()\n",
    "obj(sequence,query)\n",
    "obj=AttentionPoolingLayer(return_score=True)\n",
    "tmp=obj(sequence,query,mask_bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ad63d4b4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from layers import MLP\n",
    "from tensorflow.keras import layers\n",
    "class InterestExtractLayer(layers.Layer):\n",
    "    def __init__(self, extract_units, extract_dropout=0,**kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        # 用一个mlp来计算 auxiliary loss\n",
    "    \n",
    "    def build(self,input_shape):\n",
    "        emb_size=input_shape[-1]\n",
    "        # 传统的GRU来抽取时序行为的兴趣表示  return_sequences=True: 返回上次的输出\n",
    "        self.auxiliary_mlp = MLP(1,extract_units,dropout_rate=extract_dropout)\n",
    "        self.rnn = layers.GRU(units=emb_size, activation='tanh', recurrent_activation='sigmoid', return_sequences=True)\n",
    "\n",
    "    def call(self, user_behavior, mask_bool, neg_user_behavior=None, neg_mask_bool=None):\n",
    "        \"\"\"\n",
    "            user_behavior : (2000, 40, 4)\n",
    "            mask : (2000, 40, 1)\n",
    "            neg_user_behavior : (2000, 39, 4)\n",
    "            neg_mask : (2000, 39, 1)\n",
    "        \"\"\"\n",
    "        # 将0-1遮罩变换bool\n",
    "        # mask_bool = tf.cast(tf.squeeze(mask, axis=2), tf.bool)  # (2000, 40)\n",
    "\n",
    "        gru_interests = self.rnn(user_behavior, mask=mask_bool)  # (2000, 40, 4)\n",
    "\n",
    "        # 计算Auxiliary Loss，只在负采样的时候计算 aux loss\n",
    "        if neg_user_behavior is not None:\n",
    "            # 此处用户真实行为user_behavior为图中的e，GRU抽取的状态为图中的h\n",
    "            gru_embed = gru_interests[:, 1:]  # (2000, 39, 4)\n",
    "            #neg_mask_bool = tf.cast(tf.squeeze(neg_mask, axis=2), tf.bool)  # (2000, 39)\n",
    "\n",
    "            # 正样本的构建  选取下一个行为作为正样本\n",
    "            pos_seq = tf.concat([gru_embed, user_behavior[:, 1:]], -1)  # (2000, 39, 8)\n",
    "            pos_res = self.auxiliary_mlp(pos_seq)  # (2000, 39, 1)\n",
    "            pos_res = tf.sigmoid(pos_res[neg_mask_bool])  # 选择不为0的进行sigmoid  (N, 1) ex: (18290, 1)\n",
    "            pos_target = tf.ones_like(pos_res, tf.float16)  # label\n",
    "\n",
    "            # 负样本的构建  从未点击的样本中选取一个作为负样本\n",
    "            neg_seq = tf.concat([gru_embed, neg_user_behavior], -1)  # (2000, 39, 8)\n",
    "            neg_res = self.auxiliary_mlp(neg_seq)  # (2000, 39, 1)\n",
    "            neg_res = tf.sigmoid(neg_res[neg_mask_bool])\n",
    "            neg_target = tf.zeros_like(neg_res, tf.float16)\n",
    "\n",
    "            # 计算辅助损失 二分类交叉熵\n",
    "            aux_loss = tf.keras.losses.binary_crossentropy(tf.concat([pos_res, neg_res], axis=0), tf.concat([pos_target, neg_target], axis=0))\n",
    "            aux_loss = tf.cast(aux_loss, tf.float32)\n",
    "            aux_loss = tf.reduce_mean(aux_loss)\n",
    "\n",
    "            return gru_interests, aux_loss\n",
    "\n",
    "        return gru_interests, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a580470e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'InterestExtractLayer' object has no attribute 'rnn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m neg_user_behavior\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mones((\u001b[38;5;241m20\u001b[39m, \u001b[38;5;241m39\u001b[39m, \u001b[38;5;241m4\u001b[39m))\n\u001b[1;32m      5\u001b[0m neg_mask \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39msequence_mask(tf\u001b[38;5;241m.\u001b[39mones((\u001b[38;5;241m20\u001b[39m,))\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m30\u001b[39m,\u001b[38;5;241m39\u001b[39m)\n\u001b[0;32m----> 6\u001b[0m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\u001b[43muser_behavior\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[5], line 24\u001b[0m, in \u001b[0;36mInterestExtractLayer.call\u001b[0;34m(self, user_behavior, mask_bool, neg_user_behavior, neg_mask_bool)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;124;03m    user_behavior : (2000, 40, 4)\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;124;03m    mask : (2000, 40, 1)\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;124;03m    neg_user_behavior : (2000, 39, 4)\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;124;03m    neg_mask : (2000, 39, 1)\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# 将0-1遮罩变换bool\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# mask_bool = tf.cast(tf.squeeze(mask, axis=2), tf.bool)  # (2000, 40)\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m gru_interests \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrnn\u001b[49m(user_behavior, mask\u001b[38;5;241m=\u001b[39mmask_bool)  \u001b[38;5;66;03m# (2000, 40, 4)\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# 计算Auxiliary Loss，只在负采样的时候计算 aux loss\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m neg_user_behavior \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;66;03m# 此处用户真实行为user_behavior为图中的e，GRU抽取的状态为图中的h\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'InterestExtractLayer' object has no attribute 'rnn'"
     ]
    }
   ],
   "source": [
    "obj=InterestExtractLayer([128,128],0.2)\n",
    "user_behavior=tf.ones((20, 40, 4))\n",
    "mask=tf.sequence_mask(tf.ones((20,))*30,40)\n",
    "neg_user_behavior=tf.ones((20, 39, 4))\n",
    "neg_mask = tf.sequence_mask(tf.ones((20,))*30,39)\n",
    "obj.call(user_behavior,mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "16c02db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "class DynamicMultiRNN(layers.Layer):\n",
    "    def __init__(self, num_units=None, rnn_type='LSTM', return_sequence=True, num_layers=2, num_residual_layers=1, dropout_rate=0.2,\n",
    "                 forget_bias=1.0, **kwargs):\n",
    "\n",
    "        self.num_units = num_units\n",
    "        self.return_sequence = return_sequence\n",
    "        self.rnn_type = rnn_type\n",
    "        self.num_layers = num_layers\n",
    "        self.num_residual_layers = num_residual_layers\n",
    "        self.dropout = dropout_rate\n",
    "        self.forget_bias = forget_bias\n",
    "        super(DynamicMultiRNN, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # Create a trainable weight variable for this layer.\n",
    "        input_seq_shape = input_shape[0]\n",
    "        if self.num_units is None:\n",
    "            self.num_units = input_seq_shape.as_list()[-1]\n",
    "        if self.rnn_type == \"LSTM\":\n",
    "            single_cell = tf.nn.rnn_cell.BasicLSTMCell(self.num_units, forget_bias=self.forget_bias)\n",
    "        elif self.rnn_type == \"GRU\":\n",
    "            single_cell = tf.nn.rnn_cell.GRUCell(self.num_units)\n",
    "        else:\n",
    "            raise ValueError(\"Unknown unit type %s!\" % self.rnn_type)\n",
    "        dropout = self.dropout if tf.keras.backend.learning_phase() == 1 else 0\n",
    "        single_cell = tf.nn.rnn_cell.DropoutWrapper(cell=single_cell, input_keep_prob=(1.0 - dropout))\n",
    "        cell_list = []\n",
    "        for i in range(self.num_layers):\n",
    "            residual = (i >= self.num_layers - self.num_residual_layers)\n",
    "            if residual:\n",
    "                single_cell_residual = tf.nn.rnn_cell.ResidualWrapper(single_cell)\n",
    "                cell_list.append(single_cell_residual)\n",
    "            else:\n",
    "                cell_list.append(single_cell)\n",
    "        if len(cell_list) == 1:\n",
    "            self.final_cell = cell_list[0]\n",
    "        else:\n",
    "            self.final_cell = tf.nn.rnn_cell.MultiRNNCell(cell_list)\n",
    "        super(DynamicMultiRNN, self).build(input_shape)\n",
    "\n",
    "    def call(self, input_list, mask=None, training=None):\n",
    "        rnn_input, sequence_length = input_list\n",
    "        with tf.name_scope(\"rnn\"), tf.variable_scope(\"rnn\", reuse=tf.AUTO_REUSE):\n",
    "            rnn_output, hidden_state = tf.nn.dynamic_rnn(self.final_cell, inputs=rnn_input, sequence_length=tf.squeeze(sequence_length),\n",
    "                                                         dtype=tf.float32, scope=self.name)\n",
    "        if self.return_sequence:\n",
    "            return rnn_output\n",
    "        else:\n",
    "            return tf.expand_dims(hidden_state, axis=1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        rnn_input_shape = input_shape[0]\n",
    "        if self.return_sequence:\n",
    "            return rnn_input_shape\n",
    "        else:\n",
    "            return (None, 1, rnn_input_shape[2])\n",
    "\n",
    "    def get_config(self, ):\n",
    "        config = {'num_units': self.num_units, 'rnn_type': self.rnn_type, 'return_sequence':self.return_sequence, 'num_layers': self.num_layers,\n",
    "                  'num_residual_layers': self.num_residual_layers, 'dropout_rate': self.dropout}\n",
    "        base_config = super(DynamicMultiRNN, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef2bb71a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfv2",
   "language": "python",
   "name": "tfv2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
